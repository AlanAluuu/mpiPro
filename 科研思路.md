终于有时间来整理我所做的科研了

#########################################################

######################这是一个分割线########################

#########################################################

现在很多大规模的机器学习任务分布式计算平台上运行，比如Spark那样的现代分布式系统，和MapReduce那样的分布式并行计算框架十分热门，得到了广泛的应用。分布式算法有三个不同的阶段：存储、节点之间的通信和节点的计算阶段。而这里所关注的点其实就在于使用codes（编码）来改进分布式学习算法的通信和计算阶段。换句话说就是以**编码理论的视角来加速分布式机器学习**，使用编码提高分布式机器学习中存在的系统噪声的鲁棒性。

### 1、局限点

下面的图主要展示了分布式学习算法通信和计算阶段所存在的局限点：

 ![img](https://github.com/AlanAluuu/mpiPro/blob/main/photos/%E7%A7%91%E7%A0%94%E6%80%9D%E8%B7%AF1.png)

首先的先说通信阶段，通信阶段的局限点在于哪？这就涉及到一个概念：数据变换(**data shufflfling**)。

很多分布式算法流程大概就是：数据集在节点的之间进行一种分割，比如A矩阵就可以划分为A1，A2，A3，A4四个子矩阵。每个工作节点会在局部训练一个模型，然后将局部模型平均，并重复这个过程。当使用分布式学习算法训练模型时，通常会随机重新data shufflfling多次。每次shufflfling后，节点会从数据集中获得一个新的子矩阵的样本，这会让实验结果具有很好的统计性能。然但是虽然它具有很好的统计性能，但是他的局限点就在于：每次执行一个data shufflfling时，整个数据集都通过节点网络进行通信，就会导致由于大量的通信成本。



再谈到计算阶段，这里主要设计到的计算是矩阵乘法(matrix multiplication)。为什么要强调是矩阵乘法呢？因为矩阵乘法是最基本的线性运算之一，是许多机器学习和数据分析算法的主力。但是工作节点在计算阶段存在一个问题，那就是**straggler的问题。**什么是straggler呢？在分布式计算中，straggler是指那些执行速度比其他任务慢很多的任务。当一个分布式计算系统将一批任务分配给不同的计算节点进行处理时，由于网络延迟、硬件故障或其他原因，可能会出现某些节点上的任务执行速度远低于其他节点，导致整个作业的完成时间被拖延。

 

### 2、解决方案

上面已经提出了通信阶段和计算阶段存在的问题，那么应该如何解决上述所存在的问题？下面我们以简单的两个例子讲解如何解决上面的两个局限点。

首先是通信阶段的data shufflfling，考虑一个具有两个工作节点和一个主节点的系统。假设数据集由4批A1，...，A4组成，存储在两个工作节点中，如图3所示。

 ![img](https://github.com/AlanAluuu/mpiPro/blob/main/photos/%E7%A7%91%E7%A0%94%E6%80%9D%E8%B7%AF2.png)

进行data shuffling时，master的目标是要将A3传输给W1，将A2传输给W2。在常见的做法下，主节点需要分别传输A2和A3。但是我所看的论文提出了一种新的思想，主节点可以简单地将已经编码的消息A2 + A3广播给所有的工作节点，因为工作节点**可以使用存储数据解码处所需的数据**，即由于节点1能够访问A2，所以它可以从接收到的消息A2 + A3中减去A2，得到A3。类似地，节点2可以使用A3获得A2。如果这里假设向所有工作节点发送多播消息的成本与其中一个工作节点发送单播的消息成本完全相同，那么与主节点分别传输A2和A3（即未编码）方案相比，这种编码data shufflfling方案节省了50%的通信成本。 

 

下面主要讲一下论文是如何进行Coded Computation来解决**straggler的问题。**

下面的系统演示了一个有三个工作节点，一个主节点的分布式系统，系统的任务即计算矩阵乘法AX。把矩阵A被分为两个子矩阵A1和A2，工作节点W1存储子矩阵A1，工作节点W2存储子矩阵A2，工作节点W3存储子矩阵A1与子矩阵A2的和。

 ![img](https://github.com/AlanAluuu/mpiPro/blob/main/photos/%E7%A7%91%E7%A0%94%E6%80%9D%E8%B7%AF3.png)

 

主节点将X发送给工作节点后，每个节点计算存储的矩阵与接收到的矩阵X的矩阵乘法，并将计算结果发送回主节点。这种对原始矩阵A的分配使得主节点一旦收到任意两个计算结果，就可以计算AX。假设主节点先接收到了来自W1和W3的结果，认定此刻W2就是一个straggler，使用W3的结果减去W1的结果就可以得到W2的计算结果，再让W1和W2进行一个拼接就得到了原始的计算结果。

这种思路的核心其实就是**在分布式算法的子任务中引入冗余**，使**原始任务的结果可以从子任务结果的一个子集中解码**，将未完成的子任务视为erasures。为了达到这个特定的目的，我们使用erasures codes来设计编码的子任务。An erasure code是一种**通过引入信息冗余性的方法，以提高对噪声也就是straggler的鲁棒性**。它将有k个symbols的message编码为n个编码符号的较长的消息，以便可以通过解码编码符号[78]、[79]的子集来恢复原始的k个消息符号(It encodes a message of k symbols into a longer message of n coded symbols such that the original k message symbols can be recovered by decoding a subset of coded symbols)。从信息论的角度来看，它相当于是一种信息压缩的逆过程。 

### 3、编码分布式算法

这里我们着重介绍一下编码分布式算法的构成：局部函数、局部数据块、可解码索引集和解码函数。局部函数和数据块指定了原始计算任务和输入数据在n个工作节点中的分布方式；可解码索引集和解码函数，只要收集任何可解码集的局部计算结果，使用该解码函数就能正确恢复所期望的计算结果。

 ![img](https://github.com/AlanAluuu/mpiPro/blob/main/photos/%E7%A7%91%E7%A0%94%E6%80%9D%E8%B7%AF4.png)

解码函数取一系列索引和一系列子任务结果，如果给出任何可解码的索引集及其对应的结果，就可以正确输出目标计算总任务fA (x)。假设第i个工作节点存储第i个（编码的）数据块Ai。

**具体流程**就是：在接收到输入参数x后，主节点将x多播给所有的工作节点，然后等待，直到它接收到来自任何可解码集的响应。每个工作节点在接收到输入参数后使用本地函数计算，并将任务结果发送给主节点。一旦主节点接收到来自某些可解码集的结果，它就会对接收到的任务结果进行解码并获得fA (x)。

 ![img](https://github.com/AlanAluuu/mpiPro/blob/main/photos/%E7%A7%91%E7%A0%94%E6%80%9D%E8%B7%AF5.png)

上面这个例子就是使用一个（n，n−1）MDS代码。这里的n就相当于3，我们可以使用一个（n，k）MDS代码来推广所描述的算法，算法的流程如下所示。

我们可以使用一个（n，k）MDS代码来推广所描述的算法，如下所示。

1、对于任意1≤k≤n，数据矩阵A首先被划分为k个等大小的子矩阵。然后，通过对子矩阵中的每个元素应用一个（n，k）个MDS码，得到n个编码的子矩阵。

2、我们用A 1’，A 2’，...，A n’来表示这些n个编码的子矩阵。注意，如果编码过程使用系统的MDS代码，则1≤i≤n的Ai’= Ai。

3、在接收到任意k个任务结果后，主节点可以使用解码算法来解码k个任务结果。然后，只需通过连接它们，就可以找到AX。

算法的一个结论就是：通过编码计算，我们将证明与其他未编码的算法相比，该算法的运行时间可以显著减少。如果工作人员的数量是n，并且每个子任务的运行时间都是指数级的，最优编码矩阵乘法比未编码矩阵乘法快logn数量级的。

 

###  四、算法思路

下面是算法的一些思路

 ![img](https://github.com/AlanAluuu/mpiPro/blob/main/photos/%E7%A7%91%E7%A0%94%E6%80%9D%E8%B7%AF6.png)

计算的主要任务就是AX。这里主要通过MPI实现并行通信。MPI 是一个通信协议，它可以允许多个进程在不同的计算机之间之间进行通信和数据交换，从而完成并行计算任务。因为我是使用C++进行一个代码的编写，所以主要使用的就是mpich库函数。如果要实现MPI的一个并行通信，那就要进行一个基本的操作，就比如说初始化 MPI 环境(MPI_Init), 然后获取与进程相关的通信子中的进程总数(MPI_Comm_size),  获取当前进程在通信子中的排名(MPI_Comm_rank)等等的,然后根据据不同的进程rank执行具体的一个函数。

然后是主节点的任务, 因为要执行的是MDS码的编码, 所以要把原属数据矩阵进行一个k划分。MDS码的主要原理如下: 对于一个给定的矩阵A，MDS编码要求每个编码符号都可以由任意k个原始符号线性组合得到，并且需要满足最大距离可分离的特点。MDS 的编码本质上就是一个生成矩阵和原始矩阵的乘积, MDS 的解码实质上就是一个求逆的过程。然后主节点通过MPI的Isend函数把编码好的矩阵以及要与之计算的X按照划分发送给工作节点, 工作节点通过Recv函数接收自己要计算的划分好的矩阵以及X, 使用本地函数计算A_i * X, 对于工作节点这里可以通过一个sleep函数人为的创建几个straggler, 工作节点在计算后将任务结果发送给主节点。一旦主节点接收到来自某些可解码集的结果，它就会对接收到的任务结果进行解码(求逆)并获得AX。





